pretrained_vae_path: "./pretrained_weights/sd-vae-ft-mse"
image_encoder_path: './pretrained_weights/sd-image-variations-diffusers/image_encoder'
base_model_path: './pretrained_weights/CosmicMan-SD'
mm_path: './pretrained_weights/mm_sd_v15_v2.ckpt'

### 以下路径要改
data:
  stage1_ckpt_dir: '/apdcephfs_cq10/share_1330077/hexu/NVS/nvs_dual_branch/stage1'
  stage1_ckpt_step: 40925
  stage2_ckpt_dir: '/apdcephfs_cq10/share_1330077/hexu/NVS/nvs_dual_branch/step2_union_v1'
  stage2_ckpt_step: 27825

  # validation_dir: "/apdcephfs_cq10/share_1330077/hexu/NVS/test_set"
  validation_dir: "/apdcephfs_cq10/share_1330077/hexu/NVS-in-the-wild/test_set" # 只要改这个
  n_sample_frames: 20 

output_dir: ${data.stage2_ckpt_dir}/inference_optimization_wo_normal # 这里存视频和中间mesh

## 以上路径要改

weight_dtype: 'fp16'

noise_scheduler_kwargs:
  beta_start: 0.00085
  beta_end: 0.012
  beta_schedule: "linear"
  clip_sample: false
  steps_offset: 1
  ### Zero-SNR params
  prediction_type: "v_prediction"
  rescale_betas_zero_snr: True
  timestep_spacing: "trailing"

# 下面是新增的参数
unet_attention_mode: "read_concat_attn"  # ["read_concat_attn", "read_cross_attn"] 改
use_clip_cross_attention: false
camera_use_radius: false # 如果为true，则camera_dim=16 R+T; 如果为false，则camera_dim=9 只用R矩阵

use_semantic_guider: True # rgb用semantic
use_normal_guider: True # normal用normal

pose_guider_kwargs:
  conditioning_embedding_channels: 320
  conditioning_channels: 3
  block_out_channels: [16, 32, 96, 256]
  use_guidance_attention: False # 是否加上attention（实际上加了一个transformer3D attn0和attn1）改
  attention_num_heads: 8

unet_additional_kwargs:
  use_motion_module: true
  unet_use_temporal_attention: false
  use_camera_embedding: true
  camera_dim: 9
  # v1 近到远（只是一开始特别近）逻辑是low-level的时候捕获邻近帧，high-level的时候捕获远距离帧
  down_block_attention_indices: [[0,18,342],[0,36,324],[0,72,288],[]] # down的最后一个block没有transformer 改
  mid_block_attention_index: [0,90,180,270] # 改
  up_block_attention_indices: [[],[0,72,288],[0,36,324],[0,18,342]] # 改
  # 下面是branch新增的参数
  branch_num: 1
  copy_first_n_block: 1
  copy_last_n_block: 1
  fusion: "avg" # ["avg", "learn", "sum"]
  # 下面是animate-anyone原本的
  use_inflated_groupnorm: true
  unet_use_cross_frame_attention: false # 默认是None 其实没用到
  motion_module_resolutions: # 指的是下采样几倍的block要用motion module
  - 1
  - 2
  - 4
  - 8
  motion_module_mid_block: true 
  motion_module_decoder_only: false
  motion_module_type: Vanilla
  motion_module_kwargs:
    num_attention_heads: 8
    num_transformer_block: 1
    attention_block_types:
    - Temporal_Self
    - Temporal_Self
    temporal_position_encoding: true
    # temporal_position_encoding_max_len: 8 # pos_enc的长度 代码中会用n_sample_frames更新！
    temporal_position_encoding_type: RPE # [Vanilla, RPE, RoPE]
    temporal_attention_dim_div: 1
